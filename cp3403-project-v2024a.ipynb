{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5793796,"sourceType":"datasetVersion","datasetId":199387}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-27T00:27:52.912536Z","iopub.execute_input":"2024-02-27T00:27:52.913460Z","iopub.status.idle":"2024-02-27T00:27:52.918612Z","shell.execute_reply.started":"2024-02-27T00:27:52.913424Z","shell.execute_reply":"2024-02-27T00:27:52.917533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Your team\n* Joshua Finch\n* Kyle Ormonde\n* Ingrid Moe","metadata":{}},{"cell_type":"markdown","source":"# Task 1: Dataset","metadata":{}},{"cell_type":"markdown","source":"The first step is to find your own domain-specific dataset for your data mining project. The dataset should be complex enough so that it is not straightforward to find patterns with simple calculations (impossible without preprocessing and data mining approaches). There is no limit in size for the dataset, but typically a good sized data for mining is around 100k-100M. Minimum of 100k samples/rows and minimum of 100 attributes/columns. \n\nIt could have thousands/millions rows (or columns or sometimes both rows/columns). A good data typically contains various types of data (numerical, nominal, ordinal, Boolean etc) with some errors (missing or dirty values etc) in the data. The dataset could be text data, tabular formatted data, georeferenced data. See possible data sources: Kaggle repository (https://www.kaggle.com/datasets).","metadata":{}},{"cell_type":"code","source":"# todo: import dataset(s) into pandas and print samples.\nfpath = '/kaggle/input/us-accidents/US_Accidents_March23.csv'\ndf = pd.read_csv(fpath, header=None)\n# Print sample rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-27T00:27:55.985980Z","iopub.execute_input":"2024-02-27T00:27:55.986508Z","iopub.status.idle":"2024-02-27T00:30:33.940263Z","shell.execute_reply.started":"2024-02-27T00:27:55.986464Z","shell.execute_reply":"2024-02-27T00:30:33.939356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-02-27T00:47:33.608826Z","iopub.execute_input":"2024-02-27T00:47:33.609381Z","iopub.status.idle":"2024-02-27T00:47:33.635348Z","shell.execute_reply.started":"2024-02-27T00:47:33.609333Z","shell.execute_reply":"2024-02-27T00:47:33.633636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Set Overview\n## What is it?\nThis dataset is a countrywide car accident dataset that covers 49 states of the USA from February 2016 to March 2023 containing 7.7 million records.\n\n## Points of Interest:\nThe dataset contains a broad spectrum of information regarding all aspects of car accidents such as length of the road affected by the accident, duration, time, and severity. It also contains highly detailed weather data such as temperature, wind direction, wind speed, and precipitation. Additionally, the data set categorises different configurations of roads such as if they contain a give-way, a railway, roundabouts, junctions, etc.\n\n## Learning Goals:\nThe owner of this dataset has hopes that it can be used for a vast amount applications such as real-time accident prediction, studying accident hotspot locations, casualty analysis and potentially even identifying cause and effect rules to predict accidents.(Sobhan Moosavi, January 2021)\nWe hope to be able to identify which combination of driving conditions, outside of the obvious ones, are more likely to result in an accident in order to know what to be more aware of.","metadata":{}},{"cell_type":"markdown","source":"# Task 2: business scenarios","metadata":{}},{"cell_type":"markdown","source":"List one or more possible questions you would like to investigate using your dataset. You may start this project with one set of questions but (after exploring the dataset) finish with a new set of questions and answers.","metadata":{}},{"cell_type":"code","source":"from pandas.api.types import is_numeric_dtype\n\n# For each column, if the column is categorical, print a count of each data category\nfor col in df.columns:\n    if is_numeric_dtype(df[col]) == False:\n        print(df[col].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-02-27T00:48:55.563688Z","iopub.execute_input":"2024-02-27T00:48:55.564147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some python code and results to support your business cases\n df.describe(include='all')  # see prac-1. \n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T01:01:15.688309Z","iopub.status.idle":"2024-02-27T01:01:15.689726Z","shell.execute_reply.started":"2024-02-27T01:01:15.689458Z","shell.execute_reply":"2024-02-27T01:01:15.689481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NOTE! This is a data-based project. Make sure your comments are based on printed code outputs and/or graphs.","metadata":{}},{"cell_type":"markdown","source":"# Tast-3: preprocessing","metadata":{}},{"cell_type":"markdown","source":"See prac-2. Apply one or more preprocessing techniques","metadata":{}},{"cell_type":"code","source":"# TODO: python code with outputs\n# Add some comments to explain what and why you did.\n# NOTE! unless you have a very good reason, do not drop rows nor columns\ndata = df.copy()\n\nprint('The dataset has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\nprint(data.describe())","metadata":{"execution":{"iopub.status.busy":"2024-02-27T01:20:56.478150Z","iopub.execute_input":"2024-02-27T01:20:56.479653Z","iopub.status.idle":"2024-02-27T01:24:00.543050Z","shell.execute_reply.started":"2024-02-27T01:20:56.479588Z","shell.execute_reply":"2024-02-27T01:24:00.541731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes.value_counts()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T01:47:05.252319Z","iopub.execute_input":"2024-02-27T01:47:05.252908Z","iopub.status.idle":"2024-02-27T01:47:05.266953Z","shell.execute_reply.started":"2024-02-27T01:47:05.252851Z","shell.execute_reply":"2024-02-27T01:47:05.265202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf['ID'] = pd.to_numeric(df['ID'], errors='coerce') #convert Long attribute to numeric\n\n\nprint(df.dtypes.value_counts()) #HELP","metadata":{"execution":{"iopub.status.busy":"2024-02-27T01:51:16.671748Z","iopub.execute_input":"2024-02-27T01:51:16.672253Z","iopub.status.idle":"2024-02-27T01:51:16.783384Z","shell.execute_reply.started":"2024-02-27T01:51:16.672216Z","shell.execute_reply":"2024-02-27T01:51:16.781303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_attributes = df.columns[df.dtypes != \"object\"]\ncategorical_attributes = df.columns[df.dtypes == \"object\"]\n\nprint(numeric_attributes)\nprint(categorical_attributes) # what am i doing wrong? ","metadata":{"execution":{"iopub.status.busy":"2024-02-27T01:45:09.240808Z","iopub.execute_input":"2024-02-27T01:45:09.241353Z","iopub.status.idle":"2024-02-27T01:45:09.251500Z","shell.execute_reply.started":"2024-02-27T01:45:09.241313Z","shell.execute_reply":"2024-02-27T01:45:09.250201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task-4: ARM","metadata":{}},{"cell_type":"code","source":"# Apply one or more ARM techiques, see prac-3-ARM\n# Report your results based on your code output","metadata":{"execution":{"iopub.status.busy":"2024-01-30T05:44:11.558329Z","iopub.execute_input":"2024-01-30T05:44:11.558754Z","iopub.status.idle":"2024-01-30T05:44:11.563357Z","shell.execute_reply.started":"2024-01-30T05:44:11.558722Z","shell.execute_reply":"2024-01-30T05:44:11.562476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task-5: Classification","metadata":{}},{"cell_type":"code","source":"# apply one or more classification methods, see pracs-4 and 5\n# Report your results based on your code output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task-6: Clustering","metadata":{}},{"cell_type":"code","source":"# apply one or more clustering methods, see pracs-6 and 7\n# Report your results based on your code output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task-7: Project specific and final results","metadata":{}},{"cell_type":"code","source":"# The preceding tasks may or may not produce interesting results. They are mandatory exploration tasks.\n# In this section you need to focus on anything interesting you found so far, which is specific to your data. \n# Some additional code and output may be needed here to make any final conclusions.","metadata":{},"execution_count":null,"outputs":[]}]}